{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextureDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDIMScheduler\n",
    "from TextureDiffusion.diffuser_utils import TextureDiffusionPipeline\n",
    "from TextureDiffusion.TextureDiffusion import MutualSelfAttentionControlMaskAuto\n",
    "from TextureDiffusion.TextureDiffusion_utils import regiter_attention_editor_diffusers, register_conv_control_efficient\n",
    "from torchvision.io import read_image\n",
    "from pytorch_lightning import seed_everything\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "torch.cuda.set_device(0)  # set the GPU device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real editing with TextureDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, device):\n",
    "    image = read_image(image_path)\n",
    "    image = image[:3].unsqueeze_(0).float() / 127.5 - 1.  # [-1, 1]\n",
    "    image = F.interpolate(image, (512, 512))\n",
    "    image = image.to(device)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_step = 10     # Perform query insertion in self-attention from attention_step to 50\n",
    "resnet_step = 50        # Perform feature insertion in resnet block from 0 to resnet_step\n",
    "attention_layer = 10    # Perform query insertion in self-attention from attention_layer to 15\n",
    "out_dir = \"./workdir/\"\n",
    "mask_save = False\n",
    "seed = 1\n",
    "\n",
    "dataset = [[\"data/basket.jpg\", \"A mug and a basket on the table\", \"basket\", \"Gold\"], \n",
    "           [\"data/horse.jpg\", \"a horse running in the sunset\", \"horse\", \"Cloud\"],\n",
    "           [\"data/dog.jpg\", \"a dog sitting on the ground in front of fence\", \"dog\", \"Stone\"]]\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True) \n",
    "sample_count = len(os.listdir(out_dir))\n",
    "out_dir = os.path.join(out_dir, f\"sample_{sample_count}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "seed_everything(seed) \n",
    "\n",
    "for image_path, source_prompt, target_object, texture in dataset:\n",
    "    print('-------------------------------------------------------------------------')\n",
    "    print(source_prompt)\n",
    "\n",
    "    model = TextureDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler).to(device)\n",
    "\n",
    "    prompts = [source_prompt, texture]\n",
    "    source_image = load_image(image_path, device)\n",
    "\n",
    "    # invert the source image\n",
    "    start_code, latents_list = model.invert(source_image,\n",
    "                                            \"\",\n",
    "                                            guidance_scale=7.5,\n",
    "                                            num_inference_steps=50,\n",
    "                                            return_intermediates=True)\n",
    "    start_code = start_code.expand(len(prompts), -1, -1, -1)\n",
    "\n",
    "    words = source_prompt.split()  \n",
    "    index = words.index(target_object) + 1 \n",
    "    if mask_save is False:\n",
    "        mask_save_dir = None\n",
    "    else:\n",
    "        mask_save_dir = os.path.join(out_dir, source_prompt)\n",
    "    editor = MutualSelfAttentionControlMaskAuto(start_step = attention_step, start_layer = attention_layer, ref_token_idx = [index], mask_save_dir = mask_save_dir) \n",
    "    regiter_attention_editor_diffusers(model, editor)\n",
    "\n",
    "    # inject the feature in resnet block\n",
    "    conv_injection_t = list(range(0, resnet_step))\n",
    "    register_conv_control_efficient(model, conv_injection_t)\n",
    "\n",
    "    image_TextureDiffusion = model(prompts,\n",
    "                        latents=start_code,\n",
    "                        guidance_scale=7.5,\n",
    "                        ref_intermediate_latents=latents_list)\n",
    "\n",
    "    out_image = np.concatenate((((source_image[0].permute(1,2,0).detach().cpu().numpy() * 0.5 + 0.5)*255).astype(np.uint8),\n",
    "                            (image_TextureDiffusion[-1].permute(1,2,0).detach().cpu().numpy()*255).astype(np.uint8)),1)\n",
    "    out_image = Image.fromarray(out_image)\n",
    "    out_image.save(os.path.join(out_dir, source_prompt + \"_\" + texture + \".png\"))\n",
    "\n",
    "    print(\"Syntheiszed images are saved in\", out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ldm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "587aa04bacead72c1ffd459abbe4c8140b72ba2b534b24165b36a2ede3d95042"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
